{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "005f3e0d-8bec-413b-90a4-a11249e8b1bb",
   "metadata": {},
   "source": [
    "<h4>Data and Module Importing</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb828fa-4c52-4036-bae1-029837eafe08",
   "metadata": {},
   "source": [
    "Importing the most-used functions from throughout the project and also some additional ones that will help with model analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2e5bc1f-f87a-4ec6-beee-6fe6aa562326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2864, 21)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tools\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Toby\\\\Documents\\\\Digital Futures\\\\Projects\\\\WHO Project\\\\Life Expectancy Data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597bffc3-eb32-4c7e-b94c-bec97efc81f0",
   "metadata": {},
   "source": [
    "<h4>Train-Test Split</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8959a-9bf8-410e-a919-a8dd6f0c4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = list(df.columns)\n",
    "feature_cols.remove('Life_expectancy')\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['Life_expectancy']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7366d3-feef-4b69-8425-c9e3d7f2d10c",
   "metadata": {},
   "source": [
    "<h2>Interactive Model Selection and Outputs</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d28693-7344-487c-a3fe-569e92855670",
   "metadata": {},
   "source": [
    "<h4>Behind the Scenes Workings</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba0af9f-7020-4d3b-a853-ea175d316135",
   "metadata": {},
   "source": [
    "This first function is defined to give the user the option of a full model or a sensitivity-corrected model. This handles all inputs to only return a model if it is a valid input.\n",
    "\n",
    "Following this, we can begin running some functions that will be defined later (All models must be run before this function can be called.)\n",
    "\n",
    "For both models, we send our data into a feature engineering function to clean up the data and get our columns in the right format.\n",
    "\n",
    "Following this, we then find the optimal VIF for the columns we use in each model.\n",
    "\n",
    "Finally, we complete some model analysis.\n",
    "\n",
    "This is all then output to the user to give an overview of the quality and fit of the model being used. This is helpful in showing that the final models are up to the required standards of predictiveness and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "b3b8c152-b0b1-4890-99a2-d20f8b9b73ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection():\n",
    "\n",
    "    # Error Handling\n",
    "    \n",
    "    try: model_choice = int(input(\"\"\"Do you want to run the full model (1) or run a censored model to cover sensitive data (2)?\n",
    "    Enter your option here: \"\"\"))\n",
    "    except:\n",
    "        print(\"Invalid input. Please enter either 1 or 2 to choose your model\")\n",
    "        model_selection()\n",
    "\n",
    "    if model_choice == 1:\n",
    "\n",
    "    # Model FE and defining stage\n",
    "    \n",
    "        X_train_fe = feature_eng_full(X_test)\n",
    "        model_cols = X_train_fe.columns\n",
    "\n",
    "    # Model Metrics\n",
    "        global model_state\n",
    "        model_state = \"full\"\n",
    "        modelling(model_cols)\n",
    "        print(\"\\nThe equation derived from our linear regression model is:\")\n",
    "        print()\n",
    "        print(equation) \n",
    "        print() \n",
    "\n",
    "        lin_reg = sm.OLS(y_train, X_train_fe[optimal_cols])\n",
    "        results = lin_reg.fit()\n",
    "        print(f\"For comparison, the optimal condition number found using VIF is {results.condition_number}\")\n",
    "\n",
    "\n",
    "    elif model_choice == 2:\n",
    "        \n",
    "        X_train_fe = feature_eng_full(X_test)\n",
    "        model_cols = ['const','Year', 'Alcohol_consumption', 'log_GDP', 'Adult_mortality']\n",
    "\n",
    "    # Model Metrics\n",
    "        model_state = \"sensitive\"\n",
    "        modelling(model_cols)\n",
    "        print(\"\\nThe equation derived from our linear regression model is:\")\n",
    "        print()\n",
    "        print(equation) \n",
    "        print() \n",
    "\n",
    "        lin_reg = sm.OLS(y_train, X_train_fe[optimal_cols])\n",
    "        results = lin_reg.fit()\n",
    "        print(f\"For comparison, the optimal condition number found using VIF is {results.condition_number}\")\n",
    "\n",
    "    else:\n",
    "        print(\"This is not one of the options. Please enter either 1 or 2 to choose your model\")\n",
    "        model_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "9792513e-770d-40d7-bcae-20706702e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng_full(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    # Removing autocorrelated columns\n",
    "    \n",
    "    data = data.drop(columns = ['Country', 'Economy_status_Developing', 'Infant_deaths'])\n",
    "    \n",
    "    # One hot encoding\n",
    "    \n",
    "    data = pd.get_dummies(data, columns = ['Region'], drop_first = True, prefix = 'Region', dtype=int) \n",
    "\n",
    "    # Fixing exponential relationship\n",
    "\n",
    "    data['log_GDP'] =  np.log(data['GDP_per_capita'])\n",
    "\n",
    "    # Scaling\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data[data.columns] = scaler.fit_transform(data[data.columns])\n",
    "\n",
    "    # Removing columns we are not interested in for our model\n",
    "\n",
    "    data = data.drop(columns = ['Measles', 'GDP_per_capita', 'Population_mln', 'Thinness_five_nine_years'])\n",
    "    \n",
    "    # VIF\n",
    "\n",
    "    if model_state == \"full\":\n",
    "        data_col = data.columns\n",
    "    else:\n",
    "        data_col = ['Year', 'Alcohol_consumption', 'log_GDP', 'Adult_mortality']\n",
    "    \n",
    "    calculate_vif(data[data_col])\n",
    "    \n",
    "    data = sm.add_constant(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "4faed6ae-1f88-4f96-bfdb-cedf6df22e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(X, thresh = 5.0):\n",
    "    variables = list(range(X.shape[1]))\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        # this bit uses list comprehension to gather all the VIF values of the different variables\n",
    "        vif = [variance_inflation_factor(X.iloc[:, variables].values, ix)\n",
    "               for ix in range(X.iloc[:, variables].shape[1])]\n",
    "        \n",
    "        maxloc = vif.index(max(vif)) # getting the index of the highest VIF value\n",
    "        if max(vif) > thresh:\n",
    "            del variables[maxloc] # we delete the highest VIF value on condition that it's higher than the threshold\n",
    "            dropped = True # if we deleted anything, we set the 'dropped' value to True to stay in the while loop    \n",
    "    \n",
    "    global optimal_cols \n",
    "    optimal_cols = list(X.columns[variables])\n",
    "    optimal_cols.append('const')\n",
    "\n",
    "    # We now create a global variable and assign the list of columns still in the valid set to it, remembering to add the constant back in. We can use this to check for an optimal condition number.\n",
    "    \n",
    "    return optimal_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "c03312c3-8d79-470d-8c32-d4c5d91ccc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(col):\n",
    "\n",
    "    # Modelling Stage\n",
    "    \n",
    "    lin_reg = sm.OLS(y_train, X_train_fe[col])\n",
    "    results = lin_reg.fit()\n",
    "\n",
    "    # Metrics Observations \n",
    "    \n",
    "    print(f\"\\nThe following shows the level of success our {model_state} model has with predicting life expectancy:\\n\")\n",
    "    print(f\"\"\"\n",
    "P-Values:\n",
    "\n",
    "{round(results.pvalues,3)}\n",
    "\n",
    "R-Squared:\n",
    "    \n",
    "{results.rsquared}\n",
    "    \n",
    "AIC and BIC:\n",
    "    \n",
    "{results.aic}\n",
    "{results.bic}\n",
    "    \n",
    "Condition Number:\n",
    "    \n",
    "{results.condition_number}\n",
    "\"\"\")\n",
    "\n",
    "    # Coefficients\n",
    "\n",
    "    # From stackoverflow.com - 'user idiot-tom' - this shows how to extract the coefficients from our results and insert it into a data frame. We then are able to put them into a dictionary which\n",
    "    # will be used to form the equation used for predictions.\n",
    "\n",
    "    global coef_df\n",
    "    global coefficients\n",
    "    global equation\n",
    "    coef_df = pd.read_html(results.summary().tables[1].as_html(),header=0,index_col=0)[0]\n",
    "    coefficients = coef_df['coef'].to_dict()\n",
    "\n",
    "    equation = f\"y = {coefficients['const']}\"\n",
    "    for val, key in coefficients.items():\n",
    "        if val != 'const':\n",
    "            equation += f\" + {key}*{val}\"\n",
    "    \n",
    "    # RMSE Calculations\n",
    "    \n",
    "    y_pred = results.predict(X_train_fe[col])\n",
    "    rmse = statsmodels.tools.eval_measures.rmse(y_train, y_pred)\n",
    "    print(f\"RMSE:\\n\\n{rmse}\")\n",
    "    # print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354695f7-8038-485b-8e4e-e4c8fa0b2c4b",
   "metadata": {},
   "source": [
    "<h2>Model Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ea910-e46c-45d5-b68e-c625faa192d8",
   "metadata": {},
   "source": [
    "Here, we can call the function that allows us to select which model we want to use. The entry is exception-tested and will only allow one of two inputs. Once the user has selected their chosen model, the output will represent all the important metrics that are used to check the effectiveness of the model. This output will include:\n",
    "\n",
    "* P-Values\n",
    "* R-Squared\n",
    "* AIC and BIC\n",
    "* Condition Number\n",
    "* RMSE\n",
    "* Linear Regression Equation\n",
    "* Optimal VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "508c13ca-66ab-4c16-8ad4-e5c93ae2544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to run the full model (1) or run a censored model to cover sensitive data (2)?\n",
      "    Enter your option here:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following shows the level of success our full model has with predicting life expectancy:\n",
      "\n",
      "\n",
      "P-Values:\n",
      "\n",
      "const                                   0.000\n",
      "Year                                    0.000\n",
      "Under_five_deaths                       0.000\n",
      "Adult_mortality                         0.000\n",
      "Alcohol_consumption                     0.057\n",
      "Hepatitis_B                             0.002\n",
      "BMI                                     0.000\n",
      "Polio                                   0.000\n",
      "Diphtheria                              0.014\n",
      "Incidents_HIV                           0.000\n",
      "Thinness_ten_nineteen_years             0.001\n",
      "Schooling                               0.000\n",
      "Economy_status_Developed                0.000\n",
      "Region_Asia                             0.032\n",
      "Region_Central America and Caribbean    0.000\n",
      "Region_European Union                   0.000\n",
      "Region_Middle East                      0.115\n",
      "Region_North America                    0.006\n",
      "Region_Oceania                          0.000\n",
      "Region_Rest of Europe                   0.000\n",
      "Region_South America                    0.000\n",
      "log_GDP                                 0.000\n",
      "dtype: float64\n",
      "\n",
      "R-Squared:\n",
      "    \n",
      "0.9835984532011136\n",
      "    \n",
      "AIC and BIC:\n",
      "    \n",
      "7390.805609995028\n",
      "7517.013971009005\n",
      "    \n",
      "Condition Number:\n",
      "    \n",
      "12.705236589289497\n",
      "\n",
      "RMSE:\n",
      "\n",
      "1.2025776779399533\n",
      "\n",
      "The equation derived from our linear regression model is:\n",
      "\n",
      "y = 68.868 + 0.1834*Year + -3.4189*Under_five_deaths + -5.3247*Adult_mortality + -0.089*Alcohol_consumption + -0.1266*Hepatitis_B + -0.4056*BMI + 0.3417*Polio + -0.228*Diphtheria + 0.1931*Incidents_HIV + -0.1251*Thinness_ten_nineteen_years + 0.2649*Schooling + 1.0693*Economy_status_Developed + 0.0844*Region_Asia + 0.5687*Region_Central America and Caribbean + -0.2343*Region_European Union + 0.0585*Region_Middle East + 0.0846*Region_North America + -0.2057*Region_Oceania + 0.1471*Region_Rest of Europe + 0.3875*Region_South America + 0.652*log_GDP\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toby\\AppData\\Local\\Temp\\ipykernel_31056\\4180543220.py:38: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  coef_df = pd.read_html(results.summary().tables[1].as_html(),header=0,index_col=0)[0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The indices for endog and exog are not aligned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[475], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_selection()\n",
      "Cell \u001b[1;32mIn[473], line 27\u001b[0m, in \u001b[0;36mmodel_selection\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(equation) \n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m() \n\u001b[1;32m---> 27\u001b[0m lin_reg \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mOLS(y_train, X_train_fe[optimal_cols])\n\u001b[0;32m     28\u001b[0m results \u001b[38;5;241m=\u001b[39m lin_reg\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor comparison, the optimal condition number found using VIF is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mcondition_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:924\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    921\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    922\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception will be raised in the next version.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    923\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[1;32m--> 924\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    925\u001b[0m                           hasconst\u001b[38;5;241m=\u001b[39mhasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys:\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:749\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[1;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m--> 749\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    750\u001b[0m                           weights\u001b[38;5;241m=\u001b[39mweights, hasconst\u001b[38;5;241m=\u001b[39mhasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    751\u001b[0m nobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    752\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:203\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog: Float64Array \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attr\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwendog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\statsmodels\\base\\model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\statsmodels\\base\\model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m     96\u001b[0m                               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\statsmodels\\base\\model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 135\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data(endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\statsmodels\\base\\data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    672\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[0;32m    674\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m klass(endog, exog\u001b[38;5;241m=\u001b[39mexog, missing\u001b[38;5;241m=\u001b[39mmissing, hasconst\u001b[38;5;241m=\u001b[39mhasconst,\n\u001b[0;32m    676\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\statsmodels\\base\\data.py:89\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_constant(hasconst)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity()\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\statsmodels\\base\\data.py:533\u001b[0m, in \u001b[0;36mPandasData._check_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;66;03m# exog can be None and we could be upcasting one or the other\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    531\u001b[0m         (\u001b[38;5;28mhasattr\u001b[39m(endog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(exog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_endog\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_exog\u001b[38;5;241m.\u001b[39mindex)):\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe indices for endog and exog are not aligned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_check_integrity()\n",
      "\u001b[1;31mValueError\u001b[0m: The indices for endog and exog are not aligned"
     ]
    }
   ],
   "source": [
    "model_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef161b-fa48-44a1-b1fb-e9626d0754e2",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<h3>User Inputs Experiment</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4393ebf-88d9-44fe-96a9-e3b30969e32c",
   "metadata": {},
   "source": [
    "After a test to see how we would make our function interactive, we decided it would be best to host this portion in an online application. We thought it would be useful to keep this section in the notebook as a display of our thought process and for some transparency. It currently has no effect on the interactive function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "fcd3e6bf-e93d-4a4f-8079-6edfd6679e05",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def user_inputs_1():\n",
    "\n",
    "    # List of features included in our final full model\n",
    "    \n",
    "    user_values = ['year', 'U5 Deaths per 1000', 'adult mortality rate',\n",
    "       'alcohol consumption', 'hepatitis B immunization (%)', 'BMI', 'polio immunization (%)',\n",
    "       'diphtheria immunization (%)', 'HIV per 1000', \n",
    "       'thinness between 10-19', 'schooling years',\n",
    "       'economy status (Developed or Developing)', 'region', 'GDP']\n",
    "\n",
    "    # Initialise a dictionary to store the users inputs\n",
    "    \n",
    "    user_dict = {}\n",
    "\n",
    "    # Define lists to use in input checking - these features are grouped by the upper limit of their input\n",
    "    \n",
    "    limit1000 = ['U5 Deaths per 1000', 'adult mortality rate', 'HIV per 1000']\n",
    "    limit100 = ['hepatitis B immunization (%)', 'polio immunization (%)', 'diphtheria immunization (%)', 'thinness between 10-19']\n",
    "    #regions = \n",
    "\n",
    "    # Section used to take in user inputs\n",
    "\n",
    "    for each in user_values:       # Creates a new input for every feature in the model\n",
    "\n",
    "        # For features that require text inputs, a regular input is used. Once an input is taken, it is added into the dictionary under the respective feature.\n",
    "        \n",
    "        if each in ['economy status (Developed or Developing)', 'region']:\n",
    "            user_input = input(f\"Please enter your value for {each}: \")\n",
    "            user_dict[each] = user_input\n",
    "\n",
    "        # For features that require an integer input, there is some checking that must be done. The try exception ensures that only numbers can be used as an input and it will keep asking for\n",
    "        # a number until the user inputs one.\n",
    "        \n",
    "        else:\n",
    "            while True:\n",
    "                try:\n",
    "                    user_input = int(input(f\"Please enter your value for {each}: \"))\n",
    "\n",
    "                    # This section is used for data checking too. Some features have maximum limits for them to make sense (E.g. You cannot have 101% of a population!). This is where\n",
    "                    # the inputs are checked to ensure they follow the rules.\n",
    "                    \n",
    "                    if each in limit100 and user_input > 100:\n",
    "                        print(f\"{each.title()} must be less than 100, please enter a new value\")\n",
    "                    elif each in limit1000 and user_input > 1000:\n",
    "                        print(f\"{each.title()} must be less than 1000, please enter a new value\")\n",
    "\n",
    "                    # Once the user has entered a valid input, it is entered into the dictionary and the checks are ended. It then moves onto the next input (or ends if it is the last input)\n",
    "                    \n",
    "                    else:\n",
    "                        user_dict[each] = user_input\n",
    "                        break\n",
    "                except:\n",
    "                    print(\"This must be an integer, try again\")\n",
    "\n",
    "    # Prints the dictionary of the user's inputs\n",
    "            \n",
    "    for a, b in user_dict.items():\n",
    "        print(f\"{a.title()}: {b}\")\n",
    "    global input_df\n",
    "    input_df = pd.DataFrame(columns=[\n",
    "    'Year', 'Under_five_deaths', 'Adult_mortality',\n",
    "       'Alcohol_consumption', 'Hepatitis_B', 'BMI', 'Polio', 'Diphtheria',\n",
    "       'Incidents_HIV', 'Thinness_ten_nineteen_years', 'Schooling',\n",
    "       'Economy_status_Developed', 'Region', 'GDP_per_capita'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54651344-e1a6-4f74-8793-a72699ff157a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "user_inputs_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b6625-37a4-47f7-ba01-167ffb1aa584",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "input_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
