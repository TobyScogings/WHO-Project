{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "005f3e0d-8bec-413b-90a4-a11249e8b1bb",
   "metadata": {},
   "source": [
    "<h4>Data and Module Importing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2e5bc1f-f87a-4ec6-beee-6fe6aa562326",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2864, 21)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tools\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Toby\\\\Documents\\\\Digital Futures\\\\Projects\\\\WHO Project\\\\Life Expectancy Data.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597bffc3-eb32-4c7e-b94c-bec97efc81f0",
   "metadata": {},
   "source": [
    "<h4>Train-Test Split</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8959a-9bf8-410e-a919-a8dd6f0c4471",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = list(df.columns)\n",
    "feature_cols.remove('Life_expectancy')\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['Life_expectancy']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7366d3-feef-4b69-8425-c9e3d7f2d10c",
   "metadata": {},
   "source": [
    "<h2>Interactive Model Selection and Outputs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69df860-815f-4461-82b0-ac8fc7ce3b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d28693-7344-487c-a3fe-569e92855670",
   "metadata": {},
   "source": [
    "<h4>Behind the Scenes Workings</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b3b8c152-b0b1-4890-99a2-d20f8b9b73ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection():\n",
    "\n",
    "    # Error Handling\n",
    "    \n",
    "    try: model_choice = int(input(\"\"\"Do you want to run the full model (1) or run a censored model to cover sensitive data (2)?\n",
    "    Enter your option here: \"\"\"))\n",
    "    except:\n",
    "        print(\"Invalid input. Please enter either 1 or 2 to choose your model\")\n",
    "        model_selection()\n",
    "\n",
    "    if model_choice == 1:\n",
    "\n",
    "    # Model FE and defining stage\n",
    "    \n",
    "        X_train_fe = feature_eng_full(X_train)\n",
    "        model_cols = X_train_fe.columns\n",
    "\n",
    "    # Model Metrics\n",
    "        global model_state\n",
    "        model_state = \"full\"\n",
    "        modelling(model_cols)\n",
    "        print() # Line Break\n",
    "        model_state = \"VIF optimised\"\n",
    "        modelling(optimal_cols)\n",
    "\n",
    "    elif model_choice == 2:\n",
    "        print(\"This is a placeholder for the sensitive model\")\n",
    "    else:\n",
    "        print(\"This is not one of the options. Please enter either 1 or 2 to choose your model\")\n",
    "        model_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9792513e-770d-40d7-bcae-20706702e7bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def feature_eng_full(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    # Removing autocorrelated columns\n",
    "    \n",
    "    data = data.drop(columns = ['Country', 'Economy_status_Developing', 'Infant_deaths'])\n",
    "    \n",
    "    # One hot encoding\n",
    "    \n",
    "    data = pd.get_dummies(data, columns = ['Region'], drop_first = True, prefix = 'Region', dtype=int) \n",
    "\n",
    "    # Fixing exponential relationship\n",
    "\n",
    "    data['log_GDP'] =  np.log(data['GDP_per_capita'])\n",
    "\n",
    "    # Scaling\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data[data.columns] = scaler.fit_transform(data[data.columns])\n",
    "\n",
    "    # Removing columns we are not interested in for our model\n",
    "\n",
    "    data = data.drop(columns = ['Measles', 'GDP_per_capita', 'Population_mln', 'Thinness_five_nine_years'])\n",
    "    \n",
    "    # VIF\n",
    "\n",
    "    data_col = data.columns\n",
    "    \n",
    "    calculate_vif(data[data_col])\n",
    "    \n",
    "    data = sm.add_constant(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4faed6ae-1f88-4f96-bfdb-cedf6df22e4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_vif(X, thresh = 5.0):\n",
    "    variables = list(range(X.shape[1]))\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        # this bit uses list comprehension to gather all the VIF values of the different variables\n",
    "        vif = [variance_inflation_factor(X.iloc[:, variables].values, ix)\n",
    "               for ix in range(X.iloc[:, variables].shape[1])]\n",
    "        \n",
    "        maxloc = vif.index(max(vif)) # getting the index of the highest VIF value\n",
    "        if max(vif) > thresh:\n",
    "            del variables[maxloc] # we delete the highest VIF value on condition that it's higher than the threshold\n",
    "            dropped = True # if we deleted anything, we set the 'dropped' value to True to stay in the while loop    \n",
    "    \n",
    "    global optimal_cols \n",
    "    optimal_cols = list(X.columns[variables])\n",
    "    optimal_cols.append('const')\n",
    "\n",
    "    # We now create a global variable and assign the list of columns still in the valid set to it, remembering to add the constant back in. We can use this to check for an optimal condition number.\n",
    "    \n",
    "    return optimal_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c03312c3-8d79-470d-8c32-d4c5d91ccc71",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def modelling(col):\n",
    "\n",
    "    # Modelling Stage\n",
    "    \n",
    "    lin_reg = sm.OLS(y_train, X_train_fe[col])\n",
    "    results = lin_reg.fit()\n",
    "\n",
    "    # Metrics Observations \n",
    "    \n",
    "    print(f\"\\nThe following shows the level of success our {1} model has with predicting life expectancy:\\n\")\n",
    "    print(f\"\"\"\n",
    "P-Values:\n",
    "\n",
    "{round(results.pvalues,3)}\n",
    "\n",
    "R-Squared:\n",
    "    \n",
    "{results.rsquared}\n",
    "    \n",
    "AIC and BIC:\n",
    "    \n",
    "{results.aic}\n",
    "{results.bic}\n",
    "    \n",
    "Condition Number:\n",
    "    \n",
    "{results.condition_number}\n",
    "\"\"\")\n",
    "\n",
    "    # RMSE Calculations\n",
    "    \n",
    "    y_pred = results.predict(X_train_fe[col])\n",
    "    rmse = statsmodels.tools.eval_measures.rmse(y_train, y_pred)\n",
    "    print(f\"RMSE:\\n\\n{rmse}\")\n",
    "    # print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd3e6bf-e93d-4a4f-8079-6edfd6679e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
